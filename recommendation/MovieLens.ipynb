{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "df = pd.read_csv('./ml-20m/ratings.csv')\n",
    "X = df[['userId', 'movieId']].values\n",
    "Y = df[['rating']].values\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(X, Y, test_size=0.1)\n",
    "train_dataset = TensorDataset(torch.LongTensor(train_X), torch.FloatTensor(train_Y))\n",
    "test_dataset = TensorDataset(torch.LongTensor(test_X), torch.FloatTensor(test_Y))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, num_workers=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,help='For Saving the current Model')\n",
    "args = parser.parse_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# 行列因子分解の内積をとる代わりにMLPを通して非線形化\n",
    "class NeuralMatrixFactorization(nn.Module):\n",
    "    def __init__(self, max_user, max_item, user_k=10, item_k=10, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(max_user, user_k, 0)\n",
    "        self.item_emb = nn.Embedding(max_item, item_k, 0)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(user_k + item_k, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        user_idx = x[:, 0]\n",
    "        item_idx = x[:, 1]\n",
    "        user_feature = self.user_emb(user_idx)\n",
    "        item_feature = self.item_emb(item_idx)\n",
    "        # ユーザー特徴量と商品特徴量をまとめて一つのベクトルにする\n",
    "        out = torch.cat([user_feature, item_feature], 1)\n",
    "        # まとめた特徴量ベクトルをMLPに入れる\n",
    "        # 内積をしないので、dot((n, k),(k, m))の様な特徴量ベクトルの次元をそろえる必要がない\n",
    "        # バッチノーマリゼーションのようなニューラルネットの訓練のテクニックがそのまま使える\n",
    "        out = self.mlp(out)\n",
    "        out = nn.functional.sigmoid(out) * 5\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user, max_item = X.max(0)\n",
    "max_user = int(max_user)\n",
    "max_item = int(max_item)\n",
    "net = NeuralMatrixFactorization(max_user+1, max_item+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as V\n",
    "from statistics import mean\n",
    "def mae(x, y):\n",
    "    # 平均絶対誤差を計算\n",
    "    return (x - y).abs().mean()\n",
    "\n",
    "def eval_net(net, loader, score_fn=mae):\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y in loader:\n",
    "        x = V(x)\n",
    "        ys.append(y)\n",
    "        ypred = net(x).data\n",
    "        ypreds.append(ypred)\n",
    "    score = score_fn(torch.cat(ys).squeeze(), torch.cat(ypreds))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    #初期設定\n",
    "    #GPUが使えるか確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス\", device)\n",
    "    #モデルをGPUへ\n",
    "    net.to(device)\n",
    "    #ネットワークがある程度固定であれば高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-----------------------------------')\n",
    "        #epochごとの学習と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train() #モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval() #モデルを検証モードに\n",
    "            epoch_loss = 0.0 #epochの損失0\n",
    "            epoch_corrects = 0 #epochの正解数\n",
    "            print(dataloders_dict[phase])\n",
    "            #データローダーからミニバッチを取り出すループ\n",
    "            for inputs, labels in tqdm(dataloders_dict[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                #optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "                #順伝搬(forward)計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)#損失を計算\n",
    "                    _, preds = torch.max(outputs, 1) #ラベルを予測\n",
    "                    #訓練時はバックプロパゲーション\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    #イテレーション結果の計算\n",
    "                    #lossの合計を更新\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    #正解の合計数を更新\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #epochごとのlossと正解率を表示\n",
    "            epoch_loss = epoch_loss / len(dataloders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloders_dict[phase].dataset)\n",
    "\n",
    "            print('{} Loss:{:.4f} Acc: {:.4f}'.format(phase, epoch_loss,epoch_acc))\n",
    "            if phase == 'train':\n",
    "                train_acc_list.append(epoch_acc)\n",
    "                train_loss_list.append(epoch_loss)\n",
    "            else:\n",
    "                val_acc_list.append(epoch_acc)\n",
    "                val_loss_list.append(epoch_loss)\n",
    "\n",
    "        if(((epoch + 1) % 10) == 0):\n",
    "            save_path = './weights_' + str(epoch) + 'epoch.pth'\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "    return val_loss_list,train_loss_list, val_acc_list, train_acc_list"
   ]
  }
 ]
}