{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# ワインデータ　ダウンロード"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from urllib.request import urlretrieve\nurl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\nsavepath = \"./winequality-white.csv\"\nurlretrieve(url,savepath)",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "('./winequality-white.csv', <http.client.HTTPMessage at 0x7fc32131ac88>)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# ライブラリ読み込み"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch import optim, nn\nimport pandas as pd\nimport tqdm\nimport numpy as np",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# ワインデータ読み込み&表示\n\nワインデータの内容について\n\n| |説明|日本語の説明|\n|-------|-------|-------|\n|1|fixed acidity|酸性度|\n|2|volatile acidity|揮発性酸度|\n|3|citric acid|クエン酸|\n|4|residual sugar|残留糖|\n|5|chlorides|塩化物|\n|6|free sulfur dioxide|遊離二酸化硫黄|\n|7|total sulfur dioxide|総二酸化硫黄|\n|8|density|密度|\n|9|pH|pH|\n|10|sulphates|硫酸塩|\n|11|alcohol|アルコール|\n|12|quality|品質(0:悪い-10:良い)\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "wine_df = pd.read_csv(\"./winequality-white.csv\", sep=\";\", encoding=\"utf-8\")\nwine_df",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.270</td>\n      <td>0.36</td>\n      <td>20.70</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.00100</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.300</td>\n      <td>0.34</td>\n      <td>1.60</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.99400</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.280</td>\n      <td>0.40</td>\n      <td>6.90</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.99510</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.100000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.230</td>\n      <td>0.32</td>\n      <td>8.50</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.99560</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.230</td>\n      <td>0.32</td>\n      <td>8.50</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.99560</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8.1</td>\n      <td>0.280</td>\n      <td>0.40</td>\n      <td>6.90</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.99510</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.100000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.2</td>\n      <td>0.320</td>\n      <td>0.16</td>\n      <td>7.00</td>\n      <td>0.045</td>\n      <td>30.0</td>\n      <td>136.0</td>\n      <td>0.99490</td>\n      <td>3.18</td>\n      <td>0.47</td>\n      <td>9.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.0</td>\n      <td>0.270</td>\n      <td>0.36</td>\n      <td>20.70</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.00100</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.3</td>\n      <td>0.300</td>\n      <td>0.34</td>\n      <td>1.60</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.99400</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8.1</td>\n      <td>0.220</td>\n      <td>0.43</td>\n      <td>1.50</td>\n      <td>0.044</td>\n      <td>28.0</td>\n      <td>129.0</td>\n      <td>0.99380</td>\n      <td>3.22</td>\n      <td>0.45</td>\n      <td>11.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.1</td>\n      <td>0.270</td>\n      <td>0.41</td>\n      <td>1.45</td>\n      <td>0.033</td>\n      <td>11.0</td>\n      <td>63.0</td>\n      <td>0.99080</td>\n      <td>2.99</td>\n      <td>0.56</td>\n      <td>12.000000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8.6</td>\n      <td>0.230</td>\n      <td>0.40</td>\n      <td>4.20</td>\n      <td>0.035</td>\n      <td>17.0</td>\n      <td>109.0</td>\n      <td>0.99470</td>\n      <td>3.14</td>\n      <td>0.53</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7.9</td>\n      <td>0.180</td>\n      <td>0.37</td>\n      <td>1.20</td>\n      <td>0.040</td>\n      <td>16.0</td>\n      <td>75.0</td>\n      <td>0.99200</td>\n      <td>3.18</td>\n      <td>0.63</td>\n      <td>10.800000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6.6</td>\n      <td>0.160</td>\n      <td>0.40</td>\n      <td>1.50</td>\n      <td>0.044</td>\n      <td>48.0</td>\n      <td>143.0</td>\n      <td>0.99120</td>\n      <td>3.54</td>\n      <td>0.52</td>\n      <td>12.400000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8.3</td>\n      <td>0.420</td>\n      <td>0.62</td>\n      <td>19.25</td>\n      <td>0.040</td>\n      <td>41.0</td>\n      <td>172.0</td>\n      <td>1.00020</td>\n      <td>2.98</td>\n      <td>0.67</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>6.6</td>\n      <td>0.170</td>\n      <td>0.38</td>\n      <td>1.50</td>\n      <td>0.032</td>\n      <td>28.0</td>\n      <td>112.0</td>\n      <td>0.99140</td>\n      <td>3.25</td>\n      <td>0.55</td>\n      <td>11.400000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6.3</td>\n      <td>0.480</td>\n      <td>0.04</td>\n      <td>1.10</td>\n      <td>0.046</td>\n      <td>30.0</td>\n      <td>99.0</td>\n      <td>0.99280</td>\n      <td>3.24</td>\n      <td>0.36</td>\n      <td>9.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>6.2</td>\n      <td>0.660</td>\n      <td>0.48</td>\n      <td>1.20</td>\n      <td>0.029</td>\n      <td>29.0</td>\n      <td>75.0</td>\n      <td>0.98920</td>\n      <td>3.33</td>\n      <td>0.39</td>\n      <td>12.800000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7.4</td>\n      <td>0.340</td>\n      <td>0.42</td>\n      <td>1.10</td>\n      <td>0.033</td>\n      <td>17.0</td>\n      <td>171.0</td>\n      <td>0.99170</td>\n      <td>3.12</td>\n      <td>0.53</td>\n      <td>11.300000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.5</td>\n      <td>0.310</td>\n      <td>0.14</td>\n      <td>7.50</td>\n      <td>0.044</td>\n      <td>34.0</td>\n      <td>133.0</td>\n      <td>0.99550</td>\n      <td>3.22</td>\n      <td>0.50</td>\n      <td>9.500000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6.2</td>\n      <td>0.660</td>\n      <td>0.48</td>\n      <td>1.20</td>\n      <td>0.029</td>\n      <td>29.0</td>\n      <td>75.0</td>\n      <td>0.98920</td>\n      <td>3.33</td>\n      <td>0.39</td>\n      <td>12.800000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6.4</td>\n      <td>0.310</td>\n      <td>0.38</td>\n      <td>2.90</td>\n      <td>0.038</td>\n      <td>19.0</td>\n      <td>102.0</td>\n      <td>0.99120</td>\n      <td>3.17</td>\n      <td>0.35</td>\n      <td>11.000000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6.8</td>\n      <td>0.260</td>\n      <td>0.42</td>\n      <td>1.70</td>\n      <td>0.049</td>\n      <td>41.0</td>\n      <td>122.0</td>\n      <td>0.99300</td>\n      <td>3.47</td>\n      <td>0.48</td>\n      <td>10.500000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7.6</td>\n      <td>0.670</td>\n      <td>0.14</td>\n      <td>1.50</td>\n      <td>0.074</td>\n      <td>25.0</td>\n      <td>168.0</td>\n      <td>0.99370</td>\n      <td>3.05</td>\n      <td>0.51</td>\n      <td>9.300000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6.6</td>\n      <td>0.270</td>\n      <td>0.41</td>\n      <td>1.30</td>\n      <td>0.052</td>\n      <td>16.0</td>\n      <td>142.0</td>\n      <td>0.99510</td>\n      <td>3.42</td>\n      <td>0.47</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>7.0</td>\n      <td>0.250</td>\n      <td>0.32</td>\n      <td>9.00</td>\n      <td>0.046</td>\n      <td>56.0</td>\n      <td>245.0</td>\n      <td>0.99550</td>\n      <td>3.25</td>\n      <td>0.50</td>\n      <td>10.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>6.9</td>\n      <td>0.240</td>\n      <td>0.35</td>\n      <td>1.00</td>\n      <td>0.052</td>\n      <td>35.0</td>\n      <td>146.0</td>\n      <td>0.99300</td>\n      <td>3.45</td>\n      <td>0.44</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7.0</td>\n      <td>0.280</td>\n      <td>0.39</td>\n      <td>8.70</td>\n      <td>0.051</td>\n      <td>32.0</td>\n      <td>141.0</td>\n      <td>0.99610</td>\n      <td>3.38</td>\n      <td>0.53</td>\n      <td>10.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.4</td>\n      <td>0.270</td>\n      <td>0.48</td>\n      <td>1.10</td>\n      <td>0.047</td>\n      <td>17.0</td>\n      <td>132.0</td>\n      <td>0.99140</td>\n      <td>3.19</td>\n      <td>0.49</td>\n      <td>11.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7.2</td>\n      <td>0.320</td>\n      <td>0.36</td>\n      <td>2.00</td>\n      <td>0.033</td>\n      <td>37.0</td>\n      <td>114.0</td>\n      <td>0.99060</td>\n      <td>3.10</td>\n      <td>0.71</td>\n      <td>12.300000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4868</th>\n      <td>5.8</td>\n      <td>0.230</td>\n      <td>0.31</td>\n      <td>4.50</td>\n      <td>0.046</td>\n      <td>42.0</td>\n      <td>124.0</td>\n      <td>0.99324</td>\n      <td>3.31</td>\n      <td>0.64</td>\n      <td>10.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4869</th>\n      <td>6.6</td>\n      <td>0.240</td>\n      <td>0.33</td>\n      <td>10.10</td>\n      <td>0.032</td>\n      <td>8.0</td>\n      <td>81.0</td>\n      <td>0.99626</td>\n      <td>3.19</td>\n      <td>0.51</td>\n      <td>9.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4870</th>\n      <td>6.1</td>\n      <td>0.320</td>\n      <td>0.28</td>\n      <td>6.60</td>\n      <td>0.021</td>\n      <td>29.0</td>\n      <td>132.0</td>\n      <td>0.99188</td>\n      <td>3.15</td>\n      <td>0.36</td>\n      <td>11.450000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4871</th>\n      <td>5.0</td>\n      <td>0.200</td>\n      <td>0.40</td>\n      <td>1.90</td>\n      <td>0.015</td>\n      <td>20.0</td>\n      <td>98.0</td>\n      <td>0.98970</td>\n      <td>3.37</td>\n      <td>0.55</td>\n      <td>12.050000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4872</th>\n      <td>6.0</td>\n      <td>0.420</td>\n      <td>0.41</td>\n      <td>12.40</td>\n      <td>0.032</td>\n      <td>50.0</td>\n      <td>179.0</td>\n      <td>0.99622</td>\n      <td>3.14</td>\n      <td>0.60</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4873</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.32</td>\n      <td>1.60</td>\n      <td>0.030</td>\n      <td>33.0</td>\n      <td>122.0</td>\n      <td>0.99044</td>\n      <td>3.33</td>\n      <td>0.52</td>\n      <td>11.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4874</th>\n      <td>5.6</td>\n      <td>0.200</td>\n      <td>0.36</td>\n      <td>2.50</td>\n      <td>0.048</td>\n      <td>16.0</td>\n      <td>125.0</td>\n      <td>0.99282</td>\n      <td>3.49</td>\n      <td>0.49</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4875</th>\n      <td>7.4</td>\n      <td>0.220</td>\n      <td>0.26</td>\n      <td>1.20</td>\n      <td>0.035</td>\n      <td>18.0</td>\n      <td>97.0</td>\n      <td>0.99245</td>\n      <td>3.12</td>\n      <td>0.41</td>\n      <td>9.700000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4876</th>\n      <td>6.2</td>\n      <td>0.380</td>\n      <td>0.42</td>\n      <td>2.50</td>\n      <td>0.038</td>\n      <td>34.0</td>\n      <td>117.0</td>\n      <td>0.99132</td>\n      <td>3.36</td>\n      <td>0.59</td>\n      <td>11.600000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4877</th>\n      <td>5.9</td>\n      <td>0.540</td>\n      <td>0.00</td>\n      <td>0.80</td>\n      <td>0.032</td>\n      <td>12.0</td>\n      <td>82.0</td>\n      <td>0.99286</td>\n      <td>3.25</td>\n      <td>0.36</td>\n      <td>8.800000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4878</th>\n      <td>6.2</td>\n      <td>0.530</td>\n      <td>0.02</td>\n      <td>0.90</td>\n      <td>0.035</td>\n      <td>6.0</td>\n      <td>81.0</td>\n      <td>0.99234</td>\n      <td>3.24</td>\n      <td>0.35</td>\n      <td>9.500000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4879</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.533333</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4880</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.533333</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4881</th>\n      <td>5.0</td>\n      <td>0.235</td>\n      <td>0.27</td>\n      <td>11.75</td>\n      <td>0.030</td>\n      <td>34.0</td>\n      <td>118.0</td>\n      <td>0.99540</td>\n      <td>3.07</td>\n      <td>0.50</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4882</th>\n      <td>5.5</td>\n      <td>0.320</td>\n      <td>0.13</td>\n      <td>1.30</td>\n      <td>0.037</td>\n      <td>45.0</td>\n      <td>156.0</td>\n      <td>0.99184</td>\n      <td>3.26</td>\n      <td>0.38</td>\n      <td>10.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4883</th>\n      <td>4.9</td>\n      <td>0.470</td>\n      <td>0.17</td>\n      <td>1.90</td>\n      <td>0.035</td>\n      <td>60.0</td>\n      <td>148.0</td>\n      <td>0.98964</td>\n      <td>3.27</td>\n      <td>0.35</td>\n      <td>11.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4884</th>\n      <td>6.5</td>\n      <td>0.330</td>\n      <td>0.38</td>\n      <td>8.30</td>\n      <td>0.048</td>\n      <td>68.0</td>\n      <td>174.0</td>\n      <td>0.99492</td>\n      <td>3.14</td>\n      <td>0.50</td>\n      <td>9.600000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4885</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.550000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4886</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.28</td>\n      <td>5.70</td>\n      <td>0.028</td>\n      <td>45.0</td>\n      <td>121.0</td>\n      <td>0.99168</td>\n      <td>3.21</td>\n      <td>1.08</td>\n      <td>12.150000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4887</th>\n      <td>6.2</td>\n      <td>0.410</td>\n      <td>0.22</td>\n      <td>1.90</td>\n      <td>0.023</td>\n      <td>5.0</td>\n      <td>56.0</td>\n      <td>0.98928</td>\n      <td>3.04</td>\n      <td>0.79</td>\n      <td>13.000000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4888</th>\n      <td>6.8</td>\n      <td>0.220</td>\n      <td>0.36</td>\n      <td>1.20</td>\n      <td>0.052</td>\n      <td>38.0</td>\n      <td>127.0</td>\n      <td>0.99330</td>\n      <td>3.04</td>\n      <td>0.54</td>\n      <td>9.200000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4889</th>\n      <td>4.9</td>\n      <td>0.235</td>\n      <td>0.27</td>\n      <td>11.75</td>\n      <td>0.030</td>\n      <td>34.0</td>\n      <td>118.0</td>\n      <td>0.99540</td>\n      <td>3.07</td>\n      <td>0.50</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4890</th>\n      <td>6.1</td>\n      <td>0.340</td>\n      <td>0.29</td>\n      <td>2.20</td>\n      <td>0.036</td>\n      <td>25.0</td>\n      <td>100.0</td>\n      <td>0.98938</td>\n      <td>3.06</td>\n      <td>0.44</td>\n      <td>11.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4891</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.32</td>\n      <td>0.90</td>\n      <td>0.038</td>\n      <td>38.0</td>\n      <td>121.0</td>\n      <td>0.99074</td>\n      <td>3.24</td>\n      <td>0.46</td>\n      <td>10.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4892</th>\n      <td>6.5</td>\n      <td>0.230</td>\n      <td>0.38</td>\n      <td>1.30</td>\n      <td>0.032</td>\n      <td>29.0</td>\n      <td>112.0</td>\n      <td>0.99298</td>\n      <td>3.29</td>\n      <td>0.54</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4893</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.29</td>\n      <td>1.60</td>\n      <td>0.039</td>\n      <td>24.0</td>\n      <td>92.0</td>\n      <td>0.99114</td>\n      <td>3.27</td>\n      <td>0.50</td>\n      <td>11.200000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4894</th>\n      <td>6.6</td>\n      <td>0.320</td>\n      <td>0.36</td>\n      <td>8.00</td>\n      <td>0.047</td>\n      <td>57.0</td>\n      <td>168.0</td>\n      <td>0.99490</td>\n      <td>3.15</td>\n      <td>0.46</td>\n      <td>9.600000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4895</th>\n      <td>6.5</td>\n      <td>0.240</td>\n      <td>0.19</td>\n      <td>1.20</td>\n      <td>0.041</td>\n      <td>30.0</td>\n      <td>111.0</td>\n      <td>0.99254</td>\n      <td>2.99</td>\n      <td>0.46</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4896</th>\n      <td>5.5</td>\n      <td>0.290</td>\n      <td>0.30</td>\n      <td>1.10</td>\n      <td>0.022</td>\n      <td>20.0</td>\n      <td>110.0</td>\n      <td>0.98869</td>\n      <td>3.34</td>\n      <td>0.38</td>\n      <td>12.800000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4897</th>\n      <td>6.0</td>\n      <td>0.210</td>\n      <td>0.38</td>\n      <td>0.80</td>\n      <td>0.020</td>\n      <td>22.0</td>\n      <td>98.0</td>\n      <td>0.98941</td>\n      <td>3.26</td>\n      <td>0.32</td>\n      <td>11.800000</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>4898 rows × 12 columns</p>\n</div>",
            "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.0             0.270         0.36           20.70      0.045   \n1               6.3             0.300         0.34            1.60      0.049   \n2               8.1             0.280         0.40            6.90      0.050   \n3               7.2             0.230         0.32            8.50      0.058   \n4               7.2             0.230         0.32            8.50      0.058   \n5               8.1             0.280         0.40            6.90      0.050   \n6               6.2             0.320         0.16            7.00      0.045   \n7               7.0             0.270         0.36           20.70      0.045   \n8               6.3             0.300         0.34            1.60      0.049   \n9               8.1             0.220         0.43            1.50      0.044   \n10              8.1             0.270         0.41            1.45      0.033   \n11              8.6             0.230         0.40            4.20      0.035   \n12              7.9             0.180         0.37            1.20      0.040   \n13              6.6             0.160         0.40            1.50      0.044   \n14              8.3             0.420         0.62           19.25      0.040   \n15              6.6             0.170         0.38            1.50      0.032   \n16              6.3             0.480         0.04            1.10      0.046   \n17              6.2             0.660         0.48            1.20      0.029   \n18              7.4             0.340         0.42            1.10      0.033   \n19              6.5             0.310         0.14            7.50      0.044   \n20              6.2             0.660         0.48            1.20      0.029   \n21              6.4             0.310         0.38            2.90      0.038   \n22              6.8             0.260         0.42            1.70      0.049   \n23              7.6             0.670         0.14            1.50      0.074   \n24              6.6             0.270         0.41            1.30      0.052   \n25              7.0             0.250         0.32            9.00      0.046   \n26              6.9             0.240         0.35            1.00      0.052   \n27              7.0             0.280         0.39            8.70      0.051   \n28              7.4             0.270         0.48            1.10      0.047   \n29              7.2             0.320         0.36            2.00      0.033   \n...             ...               ...          ...             ...        ...   \n4868            5.8             0.230         0.31            4.50      0.046   \n4869            6.6             0.240         0.33           10.10      0.032   \n4870            6.1             0.320         0.28            6.60      0.021   \n4871            5.0             0.200         0.40            1.90      0.015   \n4872            6.0             0.420         0.41           12.40      0.032   \n4873            5.7             0.210         0.32            1.60      0.030   \n4874            5.6             0.200         0.36            2.50      0.048   \n4875            7.4             0.220         0.26            1.20      0.035   \n4876            6.2             0.380         0.42            2.50      0.038   \n4877            5.9             0.540         0.00            0.80      0.032   \n4878            6.2             0.530         0.02            0.90      0.035   \n4879            6.6             0.340         0.40            8.10      0.046   \n4880            6.6             0.340         0.40            8.10      0.046   \n4881            5.0             0.235         0.27           11.75      0.030   \n4882            5.5             0.320         0.13            1.30      0.037   \n4883            4.9             0.470         0.17            1.90      0.035   \n4884            6.5             0.330         0.38            8.30      0.048   \n4885            6.6             0.340         0.40            8.10      0.046   \n4886            6.2             0.210         0.28            5.70      0.028   \n4887            6.2             0.410         0.22            1.90      0.023   \n4888            6.8             0.220         0.36            1.20      0.052   \n4889            4.9             0.235         0.27           11.75      0.030   \n4890            6.1             0.340         0.29            2.20      0.036   \n4891            5.7             0.210         0.32            0.90      0.038   \n4892            6.5             0.230         0.38            1.30      0.032   \n4893            6.2             0.210         0.29            1.60      0.039   \n4894            6.6             0.320         0.36            8.00      0.047   \n4895            6.5             0.240         0.19            1.20      0.041   \n4896            5.5             0.290         0.30            1.10      0.022   \n4897            6.0             0.210         0.38            0.80      0.020   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                    45.0                 170.0  1.00100  3.00       0.45   \n1                    14.0                 132.0  0.99400  3.30       0.49   \n2                    30.0                  97.0  0.99510  3.26       0.44   \n3                    47.0                 186.0  0.99560  3.19       0.40   \n4                    47.0                 186.0  0.99560  3.19       0.40   \n5                    30.0                  97.0  0.99510  3.26       0.44   \n6                    30.0                 136.0  0.99490  3.18       0.47   \n7                    45.0                 170.0  1.00100  3.00       0.45   \n8                    14.0                 132.0  0.99400  3.30       0.49   \n9                    28.0                 129.0  0.99380  3.22       0.45   \n10                   11.0                  63.0  0.99080  2.99       0.56   \n11                   17.0                 109.0  0.99470  3.14       0.53   \n12                   16.0                  75.0  0.99200  3.18       0.63   \n13                   48.0                 143.0  0.99120  3.54       0.52   \n14                   41.0                 172.0  1.00020  2.98       0.67   \n15                   28.0                 112.0  0.99140  3.25       0.55   \n16                   30.0                  99.0  0.99280  3.24       0.36   \n17                   29.0                  75.0  0.98920  3.33       0.39   \n18                   17.0                 171.0  0.99170  3.12       0.53   \n19                   34.0                 133.0  0.99550  3.22       0.50   \n20                   29.0                  75.0  0.98920  3.33       0.39   \n21                   19.0                 102.0  0.99120  3.17       0.35   \n22                   41.0                 122.0  0.99300  3.47       0.48   \n23                   25.0                 168.0  0.99370  3.05       0.51   \n24                   16.0                 142.0  0.99510  3.42       0.47   \n25                   56.0                 245.0  0.99550  3.25       0.50   \n26                   35.0                 146.0  0.99300  3.45       0.44   \n27                   32.0                 141.0  0.99610  3.38       0.53   \n28                   17.0                 132.0  0.99140  3.19       0.49   \n29                   37.0                 114.0  0.99060  3.10       0.71   \n...                   ...                   ...      ...   ...        ...   \n4868                 42.0                 124.0  0.99324  3.31       0.64   \n4869                  8.0                  81.0  0.99626  3.19       0.51   \n4870                 29.0                 132.0  0.99188  3.15       0.36   \n4871                 20.0                  98.0  0.98970  3.37       0.55   \n4872                 50.0                 179.0  0.99622  3.14       0.60   \n4873                 33.0                 122.0  0.99044  3.33       0.52   \n4874                 16.0                 125.0  0.99282  3.49       0.49   \n4875                 18.0                  97.0  0.99245  3.12       0.41   \n4876                 34.0                 117.0  0.99132  3.36       0.59   \n4877                 12.0                  82.0  0.99286  3.25       0.36   \n4878                  6.0                  81.0  0.99234  3.24       0.35   \n4879                 68.0                 170.0  0.99494  3.15       0.50   \n4880                 68.0                 170.0  0.99494  3.15       0.50   \n4881                 34.0                 118.0  0.99540  3.07       0.50   \n4882                 45.0                 156.0  0.99184  3.26       0.38   \n4883                 60.0                 148.0  0.98964  3.27       0.35   \n4884                 68.0                 174.0  0.99492  3.14       0.50   \n4885                 68.0                 170.0  0.99494  3.15       0.50   \n4886                 45.0                 121.0  0.99168  3.21       1.08   \n4887                  5.0                  56.0  0.98928  3.04       0.79   \n4888                 38.0                 127.0  0.99330  3.04       0.54   \n4889                 34.0                 118.0  0.99540  3.07       0.50   \n4890                 25.0                 100.0  0.98938  3.06       0.44   \n4891                 38.0                 121.0  0.99074  3.24       0.46   \n4892                 29.0                 112.0  0.99298  3.29       0.54   \n4893                 24.0                  92.0  0.99114  3.27       0.50   \n4894                 57.0                 168.0  0.99490  3.15       0.46   \n4895                 30.0                 111.0  0.99254  2.99       0.46   \n4896                 20.0                 110.0  0.98869  3.34       0.38   \n4897                 22.0                  98.0  0.98941  3.26       0.32   \n\n        alcohol  quality  \n0      8.800000        6  \n1      9.500000        6  \n2     10.100000        6  \n3      9.900000        6  \n4      9.900000        6  \n5     10.100000        6  \n6      9.600000        6  \n7      8.800000        6  \n8      9.500000        6  \n9     11.000000        6  \n10    12.000000        5  \n11     9.700000        5  \n12    10.800000        5  \n13    12.400000        7  \n14     9.700000        5  \n15    11.400000        7  \n16     9.600000        6  \n17    12.800000        8  \n18    11.300000        6  \n19     9.500000        5  \n20    12.800000        8  \n21    11.000000        7  \n22    10.500000        8  \n23     9.300000        5  \n24    10.000000        6  \n25    10.400000        6  \n26    10.000000        6  \n27    10.500000        6  \n28    11.600000        6  \n29    12.300000        7  \n...         ...      ...  \n4868  10.800000        6  \n4869   9.800000        6  \n4870  11.450000        7  \n4871  12.050000        6  \n4872   9.700000        5  \n4873  11.900000        6  \n4874  10.000000        6  \n4875   9.700000        6  \n4876  11.600000        7  \n4877   8.800000        5  \n4878   9.500000        4  \n4879   9.533333        6  \n4880   9.533333        6  \n4881   9.400000        6  \n4882  10.700000        5  \n4883  11.500000        6  \n4884   9.600000        5  \n4885   9.550000        6  \n4886  12.150000        7  \n4887  13.000000        7  \n4888   9.200000        5  \n4889   9.400000        6  \n4890  11.800000        6  \n4891  10.600000        6  \n4892   9.700000        5  \n4893  11.200000        6  \n4894   9.600000        5  \n4895   9.400000        6  \n4896  12.800000        7  \n4897  11.800000        6  \n\n[4898 rows x 12 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Multi Layer Perceptron Network\nclass MLPNet (nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.linear1 = nn.Linear(11, 32)   \n        self.linear2 = nn.Linear(32, 32)\n        self.linear3 = nn.Linear(32, 10)\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.linear2(x)\n        x = F.relu(x)\n        x = self.linear3(x)\n        out = torch.sigmoid(x)\n        return out",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train_model(net, dataloders_dict, criterion, optimizer, num_epochs):\n    train_loss_list = []\n    train_acc_list = []\n    val_loss_list = []\n    val_acc_list = []\n    #初期設定\n    #GPUが使えるか確認\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"使用デバイス\", device)\n    #モデルをGPUへ\n    net.to(device)\n    #ネットワークがある程度固定であれば高速化させる\n    torch.backends.cudnn.benchmark = True\n    #epochのループ\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-----------------------------------')\n        #epochごとの学習と検証のループ\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                net.train() #モデルを訓練モードに\n            else:\n                net.eval() #モデルを検証モードに\n            epoch_loss = 0.0 #epochの損失0\n            epoch_corrects = 0 #epochの正解数\n            print(dataloders_dict[phase])\n            #データローダーからミニバッチを取り出すループ\n            for inputs, labels in tqdm(dataloders_dict[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                #optimizerを初期化\n                optimizer.zero_grad()\n                #順伝搬(forward)計算\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = net(inputs)\n                    loss = criterion(outputs, labels)#損失を計算\n                    _, preds = torch.max(outputs, 1) #ラベルを予測\n                    #訓練時はバックプロパゲーション\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                    #イテレーション結果の計算\n                    #lossの合計を更新\n                    epoch_loss += loss.item() * inputs.size(0)\n                    #正解の合計数を更新\n                    epoch_corrects += torch.sum(preds == labels.data)\n            #epochごとのlossと正解率を表示\n            epoch_loss = epoch_loss / len(dataloders_dict[phase].dataset)\n            epoch_acc = epoch_corrects.double() / len(dataloders_dict[phase].dataset)\n            print('{} Loss:{:.4f} Acc: {:.4f}'.format(phase, epoch_loss,epoch_acc))\n            if phase == 'train':\n                train_acc_list.append(epoch_acc)\n                train_loss_list.append(epoch_loss)\n            else:\n                val_acc_list.append(epoch_acc)\n                val_loss_list.append(epoch_loss)\n        if(((epoch + 1) % 10) == 0):\n            save_path = './weights_' + str(epoch) + 'epoch.pth'\n            torch.save(net.state_dict(), save_path)\n    return val_loss_list,train_loss_list, val_acc_list, train_acc_list",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = wine_df.drop(\"quality\", axis=1).values\ny = wine_df[\"quality\"].values\nx_train, x_test, y_train, y_test =  model_selection.train_test_split(x, y, test_size=0.2)\n\n\ntrain_dataset = TensorDataset(torch.FloatTensor(np.array(x_train)), torch.FloatTensor(np.array(x_test)))\ntest_dataset = TensorDataset(torch.FloatTtemsor(y_train), torch.FloatTensor(y_test))\n\ntrain_dataloder = DataLoader(train_dataset, batch_size=1024, num_workers=4, shuffle=True)\ntest_dataloder = DataLoader(test_dataset, batch_size=1024, num_workers=4)\n\ndataloders_dict = {\"train\": train_dataloder, \"val\":test_dataloder}\n\n#損失関数を設定\ncriterion = nn.CrossEntropyLoss()\n\n#モデル作成\nmodel = MLP()\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\nval_loss, train_loss, val_acc, train_acc = train_model(model, dataloders_dict, criterion, optimizer, num_epochs=args.epochs)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ba80ff098158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTtemsor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = wine_df.drop(\"quality\", axis=1).values\ny = wine_df[\"quality\"].values\ny[].shape",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "(4898,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "tensor([[ 6.6000,  0.2500,  0.3500,  ...,  3.1900,  0.4000, 12.8000],\n        [ 6.4000,  0.1700,  0.2700,  ...,  3.4600,  0.4200, 11.0000],\n        [ 6.6000,  0.3300,  0.3200,  ...,  3.2500,  0.5600, 10.4000],\n        ...,\n        [ 6.8000,  0.2600,  0.2400,  ...,  3.1300,  0.4700,  8.9000],\n        [ 7.1000,  0.2900,  0.3400,  ...,  3.2100,  0.4000, 10.7000],\n        [ 5.5000,  0.1700,  0.2300,  ...,  3.2800,  0.5000, 10.0000]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}